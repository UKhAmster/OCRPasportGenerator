import os
import json
import torch
import argparse
import re
from PIL import Image
from transformers import DonutProcessor, VisionEncoderDecoderModel
from jiwer import cer


def evaluate(model_path, dataset_path, task_prompt):
    print(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {model_path}...")
    processor = DonutProcessor.from_pretrained(model_path)
    model = VisionEncoderDecoderModel.from_pretrained(model_path)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"‚ö° –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    model.to(device)
    model.eval()

    metadata_file = os.path.join(dataset_path, "metadata.jsonl")
    if not os.path.exists(metadata_file):
        raise FileNotFoundError(f"–§–∞–π–ª {metadata_file} –Ω–µ –Ω–∞–π–¥–µ–Ω!")

    with open(metadata_file, "r", encoding="utf-8") as f:
        metadata = [json.loads(line) for line in f]

    total_cer = 0.0
    exact_matches = 0
    total_images = len(metadata)

    print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏—é {total_images} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")

    for idx, item in enumerate(metadata):
        image_path = os.path.join(dataset_path, item["file_name"])

        # –î–æ—Å—Ç–∞–µ–º –∏–¥–µ–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –∏–∑ —Ç–≤–æ–µ–π –º–µ—Ç–∞–¥–∞—Ç—ã
        ground_truth_dict = json.loads(item["ground_truth"])["gt_parse"]

        try:
            image = Image.open(image_path).convert("RGB")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {image_path}: {e}")
            continue

        pixel_values = processor(image, return_tensors="pt").pixel_values.to(device)
        decoder_input_ids = processor.tokenizer(
            task_prompt, add_special_tokens=False, return_tensors="pt"
        ).input_ids.to(device)

        with torch.no_grad():
            outputs = model.generate(
                pixel_values,
                decoder_input_ids=decoder_input_ids,
                max_length=768,
                pad_token_id=processor.tokenizer.pad_token_id,
                eos_token_id=processor.tokenizer.eos_token_id,
                use_cache=True,
                bad_words_ids=[[processor.tokenizer.unk_token_id]],
                return_dict_in_generate=True,
                repetition_penalty=1.2,
                num_beams=2
            )

            # 1. –ü–æ–ª—É—á–∞–µ–º —Å—ã—Ä—É—é —Å—Ç—Ä–æ–∫—É
            sequence = processor.batch_decode(outputs.sequences)[0]
            sequence = sequence.replace(processor.tokenizer.eos_token, "").replace(processor.tokenizer.pad_token, "")
            sequence = re.sub(r"^" + re.escape(task_prompt), "", sequence).strip()

            # 2. –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Å–ª–æ–≤–∞—Ä—å
            predicted_dict = processor.token2json(sequence)

            # –ï–°–õ–ò –ú–û–î–ï–õ–¨ –í–´–î–ê–õ–ê –°–´–†–û–ô JSON (–∫–∞–∫ –≤ —Ç–≤–æ–µ–º –ª–æ–≥–µ), —á–∏—Å—Ç–∏–º –∏ –ø–∞—Ä—Å–∏–º –µ–≥–æ –Ω–∞–ø—Ä—è–º—É—é:
            if "text_sequence" in predicted_dict:
                raw_text = predicted_dict["text_sequence"]
                # –ü—ã—Ç–∞–µ–º—Å—è –≤—ã—Ä–µ–∑–∞—Ç—å –≤—Å—ë –ª–∏—à–Ω–µ–µ –∏ –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ —Å–∞–º —Å–ª–æ–≤–∞—Ä—å
                try:
                    # –ò—â–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É {"gt_parse": {...}}
                    match = re.search(r'{"gt_parse":\s*({.*?})}', raw_text)
                    if match:
                        predicted_dict = json.loads(match.group(1))
                    else:
                        # –ï—Å–ª–∏ —Å–æ–≤—Å–µ–º –º—É—Å–æ—Ä
                        predicted_dict = {"error": raw_text}
                except json.JSONDecodeError:
                    predicted_dict = {"error": "JSON_Decode_Error"}

            # 3. –°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
            truth_str = json.dumps(ground_truth_dict, sort_keys=True, ensure_ascii=False)
            pred_str = json.dumps(predicted_dict, sort_keys=True, ensure_ascii=False)

            current_cer = cer(truth_str, pred_str)
            total_cer += current_cer

            if ground_truth_dict == predicted_dict:
                exact_matches += 1
                status = "‚úÖ –ò–î–ï–ê–õ–¨–ù–û"
            else:
                status = f"‚ùå –û–®–ò–ë–ö–ê (CER: {current_cer:.2f})"

            print(f"[{idx + 1}/{total_images}] {item['file_name']} | {status}")

            if ground_truth_dict != predicted_dict:
                print(f"   –û–∂–∏–¥–∞–ª–æ—Å—å: {truth_str}")
                print(f"   –ü–æ–ª—É—á–µ–Ω–æ:  {pred_str}")
                break  # –¢–æ—Ä–º–æ–∑–∏–º –Ω–∞ –ø–µ—Ä–≤–æ–π –æ—à–∏–±–∫–µ

    # –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    avg_cer = total_cer / total_images
    accuracy = (exact_matches / total_images) * 100

    print("\n" + "=" * 50)
    print(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –í–ê–õ–ò–î–ê–¶–ò–ò ({dataset_path})")
    print("=" * 50)
    print(f"–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {total_images}")
    print(f"–ò–¥–µ–∞–ª—å–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π (–¢–æ—á–Ω–æ—Å—Ç—å): {accuracy:.2f}%")
    print(f"–°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –ø–æ —Å–∏–º–≤–æ–ª–∞–º (CER): {avg_cer:.4f} (–±–ª–∏–∂–µ –∫ 0 = –ª—É—á—à–µ)")
    print("=" * 50 + "\n")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–ú–∞—Å—Å–æ–≤–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π")
    parser.add_argument('--type', type=str, choices=['passport', 'registration'], required=True)
    args = parser.parse_args()

    if args.type == "passport":
        evaluate("models_ready/donut_passport_v1", "dataset/val_passport", "<s_passport>")
    elif args.type == "registration":
        evaluate("models_ready/donut_registration_v1", "dataset/val_registration", "<s_registration>")