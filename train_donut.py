import os
import json
import argparse
import torch
from torch.utils.data import Dataset
from PIL import Image
from transformers import DonutProcessor, VisionEncoderDecoderModel, VisionEncoderDecoderConfig
from pytorch_lightning import LightningModule, Trainer
from pytorch_lightning.callbacks import ModelCheckpoint

# --- –ë–∞–∑–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ ---
MODEL_REPO = "naver-clova-ix/donut-base"
MAX_LENGTH = 768
IMAGE_SIZE = (2560, 1920)  # –í—ã—Å–æ–∫–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ, –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –º–µ–ª–∫–æ–≥–æ —à—Ä–∏—Ñ—Ç–∞ –∏ —Ä—É–∫–æ–ø–∏—Å–∏


class DonutDataset(Dataset):
    def __init__(self, dataset_path, processor):
        self.dataset_path = dataset_path
        self.processor = processor
        self.metadata = []

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata_file = os.path.join(dataset_path, "metadata.jsonl")
        if not os.path.exists(metadata_file):
            raise FileNotFoundError(f"–§–∞–π–ª {metadata_file} –Ω–µ –Ω–∞–π–¥–µ–Ω! –ó–∞–ø—É—Å—Ç–∏ create_metadata.py")

        with open(metadata_file, "r", encoding="utf-8") as f:
            for line in f:
                self.metadata.append(json.loads(line))

    def __len__(self):
        return len(self.metadata)

    def __getitem__(self, idx):
        item = self.metadata[idx]
        image_path = os.path.join(self.dataset_path, item["file_name"])
        image = Image.open(image_path).convert("RGB")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∏ (–ø–µ—Ä–µ–≤–æ–¥ –≤ —Ç–µ–Ω–∑–æ—Ä—ã)
        pixel_values = self.processor(image, return_tensors="pt").pixel_values

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ (JSON -> —Ç–æ–∫–µ–Ω—ã)
        target_sequence = item["ground_truth"]
        input_ids = self.processor.tokenizer(
            target_sequence,
            add_special_tokens=False,
            max_length=MAX_LENGTH,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        ).input_ids

        labels = input_ids.clone()
        # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º pad_token –ø—Ä–∏ –ø–æ–¥—Å—á–µ—Ç–µ Loss
        labels[labels == self.processor.tokenizer.pad_token_id] = -100

        return pixel_values.squeeze(), labels.squeeze()


class DonutModule(LightningModule):
    def __init__(self, processor, model, lr, dataset_path, batch_size):
        super().__init__()
        self.processor = processor
        self.model = model
        self.lr = lr
        self.dataset_path = dataset_path
        self.batch_size = batch_size

    def training_step(self, batch, batch_idx):
        pixel_values, labels = batch
        outputs = self.model(pixel_values, labels=labels)
        loss = outputs.loss
        self.log("train_loss", loss, prog_bar=True)
        return loss

    def configure_optimizers(self):
        return torch.optim.Adam(self.model.parameters(), lr=self.lr)

    def train_dataloader(self):
        train_dataset = DonutDataset(self.dataset_path, self.processor)
        return torch.utils.data.DataLoader(
            train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=4,
            pin_memory=True
        )


def main(args):
    print(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞: {args.dataset}")

    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞ –∏ –º–æ–¥–µ–ª–∏
    config = VisionEncoderDecoderConfig.from_pretrained(MODEL_REPO)
    config.encoder.image_size = IMAGE_SIZE
    config.decoder.max_length = MAX_LENGTH

    processor = DonutProcessor.from_pretrained(MODEL_REPO)
    model = VisionEncoderDecoderModel.from_pretrained(MODEL_REPO, config=config)

    # 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    processor.tokenizer.pad_token = processor.tokenizer.unk_token
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü-—Ç–æ–∫–µ–Ω—ã –¥–ª—è –Ω–∞—à–∏—Ö –∑–∞–¥–∞—á
    processor.tokenizer.add_special_tokens({"additional_special_tokens": ["<s_passport>", "<s_registration>"]})
    model.decoder.resize_token_embeddings(len(processor.tokenizer))

    # 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Lightning
    module = DonutModule(processor, model, args.lr, args.dataset, args.batch)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç—ã –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –ø–∞–ø–∫—É –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    checkpoint_dir = os.path.join("checkpoints", args.name)
    checkpoint_callback = ModelCheckpoint(
        dirpath=checkpoint_dir,
        filename="donut-{epoch:02d}-{train_loss:.2f}",
        save_top_k=1,  # –•—Ä–∞–Ω–∏–º —Ç–æ–ª—å–∫–æ 1 –ª—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç, —á—Ç–æ–±—ã –Ω–µ –∑–∞–±–∏—Ç—å SSD
        monitor="train_loss"
    )

    trainer = Trainer(
        accelerator="gpu",
        devices=1,
        max_epochs=args.epochs,
        precision="bf16-mixed",  # –ò–¥–µ–∞–ª—å–Ω–æ –¥–ª—è RTX 5090 (BFloat16)
        callbacks=[checkpoint_callback],
        gradient_clip_val=1.0  # –ó–∞—â–∏—Ç–∞ –æ—Ç –≤–∑—Ä—ã–≤–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    )

    # 4. –ó–∞–ø—É—Å–∫
    print(f"üöÄ –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ '{args.name}' –Ω–∞ RTX 5090...")
    trainer.fit(module)

    # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –≤–µ—Å–æ–≤
    output_model_dir = os.path.join("models_ready", args.name)
    os.makedirs(output_model_dir, exist_ok=True)
    model.save_pretrained(output_model_dir)
    processor.save_pretrained(output_model_dir)
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ '{output_model_dir}'")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–û–±—É—á–µ–Ω–∏–µ Donut OCR")
    parser.add_argument('--dataset', type=str, required=True,
                        help='–ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º (–≥–¥–µ –ª–µ–∂–∏—Ç metadata.jsonl)')
    parser.add_argument('--name', type=str, required=True, help='–ò–º—è –º–æ–¥–µ–ª–∏ (—Å–æ–∑–¥–∞—Å—Ç –ø–∞–ø–∫—É —Å —Ç–∞–∫–∏–º –∏–º–µ–Ω–µ–º)')
    parser.add_argument('--epochs', type=int, default=10, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö')
    parser.add_argument('--batch', type=int, default=4, help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (–Ω–∞ 5090 –º–æ–∂–Ω–æ 4 –∏–ª–∏ 8)')
    parser.add_argument('--lr', type=float, default=3e-5, help='Learning rate')

    args = parser.parse_args()
    main(args)