import os
import json
import argparse
import torch
from torch.utils.data import Dataset
from PIL import Image
from transformers import DonutProcessor, VisionEncoderDecoderModel, VisionEncoderDecoderConfig
from pytorch_lightning import LightningModule, Trainer
from pytorch_lightning.callbacks import ModelCheckpoint

# --- –ë–∞–∑–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (–ë–ï–ó–û–ü–ê–°–ù–´–ï –î–õ–Ø –°–¢–ê–†–¢–ê) ---
MODEL_REPO = "naver-clova-ix/donut-base"
MAX_LENGTH = 768
IMAGE_SIZE = (1280, 960)  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏!

# –£—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–ª—è Tensor Cores
torch.set_float32_matmul_precision('high')


class DonutDataset(Dataset):
    def __init__(self, dataset_path, processor):
        self.dataset_path = dataset_path
        self.processor = processor
        self.metadata = []

        metadata_file = os.path.join(dataset_path, "metadata.jsonl")
        if not os.path.exists(metadata_file):
            raise FileNotFoundError(f"–§–∞–π–ª {metadata_file} –Ω–µ –Ω–∞–π–¥–µ–Ω!")

        with open(metadata_file, "r", encoding="utf-8") as f:
            for line in f:
                self.metadata.append(json.loads(line))
        print(f"üì¶ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {len(self.metadata)} –ø—Ä–∏–º–µ—Ä–æ–≤.")

    def __len__(self):
        return len(self.metadata)

    def __getitem__(self, idx):
        item = self.metadata[idx]
        image_path = os.path.join(self.dataset_path, item["file_name"])
        image = Image.open(image_path).convert("RGB")

        pixel_values = self.processor(image, return_tensors="pt").pixel_values

        target_sequence = item["ground_truth"]
        input_ids = self.processor.tokenizer(
            target_sequence,
            add_special_tokens=False,
            max_length=MAX_LENGTH,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        ).input_ids

        labels = input_ids.clone()
        labels[labels == self.processor.tokenizer.pad_token_id] = -100

        return pixel_values.squeeze(), labels.squeeze()


class DonutModule(LightningModule):
    def __init__(self, processor, model, lr, dataset_path, batch_size):
        super().__init__()
        self.processor = processor
        self.model = model
        self.lr = lr
        self.dataset_path = dataset_path
        self.batch_size = batch_size

    def setup(self, stage=None):
        print("‚öôÔ∏è Lightning Module: –í—ã–∑–≤–∞–Ω setup() - –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –æ–±—É—á–µ–Ω–∏—é...")

    def on_train_start(self):
        print("üü¢ Lightning Module: on_train_start() - –û–±—É—á–µ–Ω–∏–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ –Ω–∞—á–∞–ª–æ—Å—å!")

    def training_step(self, batch, batch_idx):
        if batch_idx == 0:
            print("üöÄ –ü–ï–†–í–´–ô –ë–ê–¢–ß –î–û–®–ï–õ –î–û GPU! –ù–∞—á–∏–Ω–∞–µ–º –≤—ã—á–∏—Å–ª–µ–Ω–∏—è...")

        pixel_values, labels = batch
        outputs = self.model(pixel_values, labels=labels)
        loss = outputs.loss
        self.log("train_loss", loss, prog_bar=True)
        return loss

    def configure_optimizers(self):
        print("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ Adam...")
        return torch.optim.Adam(self.model.parameters(), lr=self.lr)

    def train_dataloader(self):
        print("‚öôÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ DataLoader...")
        train_dataset = DonutDataset(self.dataset_path, self.processor)
        # –í–ê–ñ–ù–û: num_workers=0 –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —Ç–∏—Ö–æ–µ –∑–∞–≤–∏—Å–∞–Ω–∏–µ –≤ –≤–∏—Ä—Ç—É–∞–ª–∫–∞—Ö!
        return torch.utils.data.DataLoader(
            train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=0,
            pin_memory=True
        )


def main(args):
    print(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞: {args.dataset}")

    print("‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏...")
    config = VisionEncoderDecoderConfig.from_pretrained(MODEL_REPO)
    config.encoder.image_size = IMAGE_SIZE
    config.decoder.max_length = MAX_LENGTH

    print("‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –∏ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏...")
    processor = DonutProcessor.from_pretrained(MODEL_REPO)
    model = VisionEncoderDecoderModel.from_pretrained(MODEL_REPO, config=config)

    processor.tokenizer.pad_token = processor.tokenizer.unk_token
    processor.tokenizer.add_special_tokens({"additional_special_tokens": ["<s_passport>", "<s_registration>"]})
    model.decoder.resize_token_embeddings(len(processor.tokenizer))

    module = DonutModule(processor, model, args.lr, args.dataset, args.batch)

    checkpoint_dir = os.path.join("checkpoints", args.name)
    checkpoint_callback = ModelCheckpoint(
        dirpath=checkpoint_dir,
        filename="donut-{epoch:02d}-{train_loss:.2f}",
        save_top_k=1,
        monitor="train_loss"
    )

    print("‚è≥ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Trainer...")
    trainer = Trainer(
        accelerator="gpu",
        devices=1,
        max_epochs=args.epochs,
        precision="16-mixed",  # –ë–ï–ó–û–ü–ê–°–ù–ê–Ø –¢–û–ß–ù–û–°–¢–¨
        callbacks=[checkpoint_callback],
        gradient_clip_val=1.0
    )

    print(f"üöÄ –ü–µ—Ä–µ–¥–∞–µ–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤ PyTorch Lightning. –ñ–¥–µ–º —Å—Ç–∞—Ä—Ç–∞ —ç–ø–æ—Ö...")
    trainer.fit(module)

    output_model_dir = os.path.join("models_ready", args.name)
    os.makedirs(output_model_dir, exist_ok=True)
    model.save_pretrained(output_model_dir)
    processor.save_pretrained(output_model_dir)
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ '{output_model_dir}'")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–û–±—É—á–µ–Ω–∏–µ Donut OCR")
    parser.add_argument('--dataset', type=str, required=True)
    parser.add_argument('--name', type=str, required=True)
    parser.add_argument('--epochs', type=int, default=10)
    parser.add_argument('--batch', type=int, default=1)  # –î–µ—Ñ–æ–ª—Ç —Ç–µ–ø–µ—Ä—å 1
    parser.add_argument('--lr', type=float, default=3e-5)

    args = parser.parse_args()
    main(args)